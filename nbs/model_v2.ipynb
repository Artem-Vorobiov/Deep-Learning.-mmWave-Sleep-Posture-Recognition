{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09960447",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset, random_split, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from valtes_radartoolbox.training.preprocessing import get_dataset_min_max, custom_collate_fn, Normalize, ScalerX, ScalerY\n",
    "from valtes_radartoolbox.training.model import ModifiedResNet\n",
    "from valtes_radartoolbox.training.evaluation import create_confusion_matrix, weighted_accuracy\n",
    "from valtes_radartoolbox.training.train import train_single_epoch, train\n",
    "from valtes_radartoolbox.data.dataset import ValtesDataset, TLVDataset, TrainDataset\n",
    "from valtes_radartoolbox.data.frame import FrameLabel\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca03eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(9)\n",
    "np.random.seed(2)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "NUM_EPOCHS = 60\n",
    "batch_size = [8, 16, 32, 64]  # Different batch sizes to test\n",
    "window_sizes = [6, 7, 8, 9, 10, 12]  # Different window sizes to test\n",
    "num_classes = 4  # Number of output classes\n",
    "\n",
    "result_acc = []  # To store results for each combination of batch size and window size\n",
    "\n",
    "# Iterate over all batch sizes and window sizes\n",
    "for i in batch_size:\n",
    "    batch_acc = []  # To store accuracy results for each window size with the current batch size\n",
    "    for j in window_sizes:\n",
    "        # Initialize datasets for training\n",
    "        dataset = TrainDataset(\"Config_4_2023-11-15_John\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset2 = TrainDataset(\"Config_4_2023-11-15_Artem\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset3 = TrainDataset(\"20231120_144541\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset4 = TrainDataset(\"20231120_145729\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset5 = TrainDataset(\"20231120_150635\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset6 = TrainDataset(\"20231120_153152\", label_set=\"labels/state_labels.csv\", window_size=j)  # Artem labels\n",
    "        dataset7 = TrainDataset(\"20231120_152424\", label_set=\"labels/state_labels.csv\", window_size=j)  # Artem labels\n",
    "\n",
    "        # Datasets for augmentation\n",
    "        dataset_augm = TrainDataset(\"Config_4_2023-11-15_John\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset2_augm = TrainDataset(\"Config_4_2023-11-15_Artem\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset3_augm = TrainDataset(\"20231120_144541\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset4_augm = TrainDataset(\"20231120_145729\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset5_augm = TrainDataset(\"20231120_150635\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset6_augm = TrainDataset(\"20231120_153152\", label_set=\"labels/state_labels.csv\", window_size=j)  # Artem labels\n",
    "        dataset7_augm = TrainDataset(\"20231120_152424\", label_set=\"labels/state_labels.csv\", window_size=j)  # Artem labels\n",
    "\n",
    "        # Datasets for augmentation with different transformation\n",
    "        dataset_augmY = TrainDataset(\"Config_4_2023-11-15_John\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset2_augmY = TrainDataset(\"Config_4_2023-11-15_Artem\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset3_augmY = TrainDataset(\"20231120_144541\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset4_augmY = TrainDataset(\"20231120_145729\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset5_augmY = TrainDataset(\"20231120_150635\", label_set=\"labels/state_labels.csv\", window_size=j)\n",
    "        dataset6_augmY = TrainDataset(\"20231120_153152\", label_set=\"labels/state_labels.csv\", window_size=j)  # Artem labels\n",
    "        dataset7_augmY = TrainDataset(\"20231120_152424\", label_set=\"labels/state_labels.csv\", window_size=j)  # Artem labels\n",
    "\n",
    "        # Combine training datasets\n",
    "        datasets_train = [dataset2, dataset3, dataset4, dataset5, dataset6, dataset2_augm, dataset3_augm, dataset4_augm, dataset5_augm, dataset6_augm, \n",
    "                          dataset2_augmY, dataset3_augmY, dataset4_augmY, dataset5_augmY, dataset6_augmY]\n",
    "        # Testing datasets\n",
    "        datasets_test = [dataset7, dataset]\n",
    "\n",
    "        # Set experiment name and initialize model\n",
    "        EXPERIMENT_NAME = f\"2023-11-22_batch_size_{i}_SGD_lr_0.001\"\n",
    "        model = ModifiedResNet(num_classes=num_classes, window_size=j, freeze_weights=False).double().to(device)\n",
    "        \n",
    "        # Define loss function, optimizer, and scheduler\n",
    "        LOSS_FN = nn.CrossEntropyLoss().double()\n",
    "        OPTIMIZER = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        SCHEDULER = torch.optim.lr_scheduler.StepLR(OPTIMIZER, step_size=30, gamma=0.1)\n",
    "\n",
    "        # Combine datasets and split into training and validation sets\n",
    "        concat_dataset_train = ConcatDataset(datasets_train)\n",
    "        concat_dataset_test = ConcatDataset(datasets_test)\n",
    "        train_length = int(0.95 * len(concat_dataset_train))\n",
    "        train_dataset, _ = random_split(concat_dataset_train, [train_length, len(concat_dataset_train)-train_length], generator=torch.Generator().manual_seed(42))\n",
    "        val_length = int(0.95 * len(concat_dataset_test))\n",
    "        val_dataset, _ = random_split(concat_dataset_test, [val_length, len(concat_dataset_test)-val_length], generator=torch.Generator().manual_seed(42))\n",
    "        \n",
    "        # Create DataLoader for validation\n",
    "        dataloader_val = DataLoader(val_dataset, batch_size=i, shuffle=True, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        # Get dataset normalization parameters and apply normalization\n",
    "        min_, max_ = get_dataset_min_max(dataloader_val)\n",
    "        normalize = Normalize(min_, max_)\n",
    "        \n",
    "        # Define and apply scaling transforms for different datasets\n",
    "        doppler_Scaler = ScalerX(1.7, j)\n",
    "        changesX = [normalize, doppler_Scaler]\n",
    "        doppler_Scaler = ScalerY(1.7, j)\n",
    "        changesY = [normalize, doppler_Scaler]\n",
    "        [dataset.set_transform(transforms.Compose([normalize])) for dataset in train_dataset.dataset.datasets[:5]]\n",
    "        [dataset.set_transform(transforms.Compose(changesX)) for dataset in train_dataset.dataset.datasets[5:10]]\n",
    "        [dataset.set_transform(transforms.Compose(changesY)) for dataset in train_dataset.dataset.datasets[10:15]]\n",
    "        [dataset.set_transform(transforms.Compose([normalize])) for dataset in val_dataset.dataset.datasets]\n",
    "\n",
    "        # Create sampler for imbalanced dataset and DataLoader for training\n",
    "        sampler = ImbalancedDatasetSampler(\n",
    "            train_dataset,\n",
    "            labels=np.concatenate([dataset.frame_labels for dataset in train_dataset.dataset.datasets])[train_dataset.indices]\n",
    "        )\n",
    "        dataloader_train = DataLoader(train_dataset, batch_size=i, sampler=sampler, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        # Train the model and record the results\n",
    "        avg_vacc, avg_wvacc = train(EXPERIMENT_NAME, model, OPTIMIZER, SCHEDULER, LOSS_FN, dataloader_train, dataloader_val, NUM_EPOCHS, i, j)\n",
    "        \n",
    "        # Store results for current batch size and window size\n",
    "        values_acc = {\"batch_size\": i, \"window_size\": j, \"avg_vacc\": avg_vacc, \"avg_wvacc\": avg_wvacc}\n",
    "        batch_acc.append(values_acc)\n",
    "        print(f\"Batch_size - {i}, Window_size - {j}\")\n",
    "    \n",
    "    result_acc.append(batch_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the pickle file\n",
    "pickle_file_path = \"3cl_augm_scalerX+Y_60epoches_3sets.pkl\"\n",
    "\n",
    "# Save the variable to a pickle file\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(result_acc, pickle_file)\n",
    "\n",
    "print(f\"Variable saved as pickle file: {pickle_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74776c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
